{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "使用bert模型对短文本数据进行embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "# 读取标题数据\n",
    "title_data = pd.read_csv(\"../data/title.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初步去重\n",
    "- 简单dropna/drop_duplicates\n",
    "- 保留长度小于512的数据\n",
    "- 微博数据清洗：去除@xxx等等\n",
    "- 正则表达式去除非汉字\n",
    "- 最后只保留非空的处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original data shape:\",title_data.shape)\n",
    "\n",
    "# 初步去重\n",
    "title_data.dropna(axis=0,how='any')\n",
    "unique_title_data = title_data.dropna(axis=0,how='any').drop_duplicates(subset='text')\n",
    "print(\"drop_duplicates data shape:\",unique_title_data.shape)\n",
    "#unique_title_data[\"text\"].str.len().hist(bins=200)\n",
    "\n",
    "# 过滤特别长的一些数据\n",
    "short_unique_title_data = unique_title_data[unique_title_data['text'].str.len()<512]\n",
    "print(\"short drop_duplicates data shape:\",short_unique_title_data.shape)\n",
    "short_unique_title_data[\"text\"].str.len().hist(bins=512)\n",
    "\n",
    "# for idx in short_unique_title_data[\"text\"].str.len().sort_values().index.tolist()[-100:]:\n",
    "#     print(idx,short_unique_title_data[\"text\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from pandarallel import pandarallel\n",
    "import os, time, random\n",
    "from weibo_preprocess_toolkit import WeiboPreprocess\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def text_preprocess(data):\n",
    "    data.replace(' ','')\n",
    "    return \n",
    "    \n",
    "# 微博数据预处理\n",
    "def data_preprocess(data):\n",
    "    preprocess = WeiboPreprocess()\n",
    "    start = time.time()\n",
    "    clean_data = data['text'].parallel_map(preprocess.clean)\n",
    "    end = time.time()\n",
    "    print('Task runs %0.2f seconds.' %(end - start))\n",
    "    return clean_data\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pandarallel.initialize()\n",
    "    psutd = short_unique_title_data.copy()\n",
    "    psutd['clean'] = data_preprocess(psutd)\n",
    "    \n",
    "#     psutd['clean'] = psutd['clean'].parallel_map(replace(' ',''))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则表达式只保留汉字\n",
    "%%time\n",
    "import re\n",
    "\n",
    "# \\s\n",
    "psutd['clean'] = [re.sub(\"[^\\u4e00-\\u9fa5]\",'',ctext) for ctext in psutd['clean'].tolist()]\n",
    "psutd = psutd[psutd['clean'].str.len()>1]\n",
    "psutd = psutd.drop_duplicates(subset='clean')\n",
    "print(\"clean data shape:\",psutd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是simhash文本去重环节\n",
    "> 因为python计算这部分比较慢，所以没有继续"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多进程结巴分词\n",
    "%%time\n",
    "import jieba\n",
    "jieba.enable_parallel(8)\n",
    "seg_list = [jieba.lcut(text) for text in psutd['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算simhash值\n",
    "%%time\n",
    "from simhash import Simhash as SH\n",
    "SH(seg_list[0]).value\n",
    "simhash_list = [SH(seg) for seg in seg_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simhash矩阵python计算过于缓慢，之后可能考虑c++/cuda调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过于缓慢\n",
    "\n",
    "# %%time\n",
    "# uset={}\n",
    "# sim_list_len = len(simhash_list)\n",
    "# flag_list = [range(sim_list_len)]\n",
    "# pair_list = []\n",
    "# for idx in range(sim_list_len):\n",
    "#     for pair in range(idx,sim_list_len):\n",
    "#         if (simhash_list[idx].distance(simhash_list[pair])<5):\n",
    "#             pair_list.append((idx,pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析\n",
    "- 数值特征分析\n",
    "- bert生成embedding\n",
    "- 并查集分析&相似矩阵分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutd['clean'].str.len().hist(bins=512)\n",
    "print(psutd['clean'].str.len().mean())\n",
    "print(psutd['clean'].str.len().median())\n",
    "print(psutd.iloc[0])\n",
    "\n",
    "# for idx in psutd[\"clean\"].str.len().sort_values().index.tolist()[-10:]:\n",
    "#     print(idx,psutd[\"clean\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入bert-as-service\n",
    "这里选择的是google-bert-base模型，在命令行启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version is\",tf.__version__)\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "# print(bc.encode(['First do it', '今天天气不错', 'then do it better']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert test\n",
    "from sklearn.metrics.pairwise import pairwise_distances as PD\n",
    "vec = bc.encode(['外交部召见美国驻华大使提出严正交涉敦促美方纠正错误停止利用涉港问题干涉中国内政中国外交部副部''今天天气不错今天天气不错今天天气不错今天天气不错今天天气不错今天天气不错','今天天气不错','亚洲球员在多重看这在上之后武磊二个赛季遭遇前所级区发机会 但是前 轮联赛颗粒无收 当然 这也与西甲联赛一属性有关 历史上能够真正立足西甲联赛的亚洲球员屈指可数 目前西甲联赛也只有中日韩 名球员效力 其馀三大亚洲球星更是只能委身西乙联赛 △目前 从西班牙职业联赛的亚洲球员看 日本球员还是占据主流 名国脚都在西甲或是西乙联赛效力 从球员基数看 日本球员整体适应能力确实了得 良好的职业态度和扎实的基本功 让他们在西班牙联','亚洲球员在西甲分量有多重在上赛季初试身手之后武磊在留洋西甲的第二个赛季遭遇前所未有的困难西班牙人队深陷降级区武磊虽然获得不少首发机会 但是前 轮联赛颗粒无收 当然 这也与西甲联赛一属性有关 历史上能够真正立足西甲联赛的亚洲球员屈指可数 目前西甲联赛也只有中日韩 名球员效力 其馀三大亚洲球星更是只能委身西乙联赛 △目前 从西班牙职业联赛的亚洲球员看 日本球员还是占据主流 名国脚都在西甲或是西乙联赛效力 从球员基数看 日本球员整体适应能力确实了得 良好的职业态度和扎实的基本功 让他们在西班牙联赛获'])\n",
    "\n",
    "print(vec)\n",
    "print(PD(vec,vec,n_jobs=8))\n",
    "matplotlib.pyplot.matshow(ED(vec,vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用bert-service服务计算，可能会花费10分钟甚至更久\n",
    "> 300K数据，max_seq_len=64，双P40耗时10分钟左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clean_vec = bc.encode(psutd[\"clean\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将向量保存为二进制数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/hk_nodes\",'wb') as bin_output:\n",
    "    clean_vec.tofile(bin_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "对全体向量进行二维PCA分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    "clean_pca2 = pca.fit_transform(clean_vec)\n",
    "matplotlib.pyplot.scatter(clean_pca2[:,0],clean_pca2[:,1],alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用邻接边计算程序，同时得到并查集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "node_num = clean_vec.shape[0]\n",
    "node_dim = clean_vec.shape[1]\n",
    "threshold = 18.0\n",
    "os.system(' '.join([\"cd ../Kluster; cd bin; ./linker ../data/hk_nodes ../data/hk_edges.csv\",str(node_num),str(node_dim),str(threshold)]))\n",
    "hk_edge = pd.read_csv(\"../Kluster/data/hk_edges..csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_edge['distance'].hist(bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析向量的相似程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "edm = PD(clean_vec[:1000],clean_vec[:1000],n_jobs=8)\n",
    "print(edm)\n",
    "matplotlib.pyplot.matshow(edm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取&分析并查集结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set(path):\n",
    "    disjoint_set={}\n",
    "    with open(path,'r') as set_file:\n",
    "        set_lines = set_file.readlines()\n",
    "    set_lines = set_lines[1:]\n",
    "    for line in set_lines:\n",
    "        line = line[:-2]\n",
    "        set_id = int(line.split(':')[0])\n",
    "        disjoint_set[set_id]=[int(node) for node in line.split(':')[1].split(',')]\n",
    "    return disjoint_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "disjoint_set = read_set(\"../data/set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(disjoint_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找出最大的并查集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "disjoint_set = read_set(\"../Kluster/data/set.txt\")\n",
    "biggest_set = 0\n",
    "bs_len = 1\n",
    "for set_id,node_list in disjoint_set.items():\n",
    "    if len(node_list)>bs_len:\n",
    "        biggest_set = set_id\n",
    "        bs_len = len(node_list)\n",
    "\n",
    "print(bs_len)\n",
    "print(disjoint_set[biggest_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到最大并查集中的项，分析其相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_vec = [clean_vec[vec_id] for vec_id in disjoint_set[biggest_set]]\n",
    "edm = ED(set_vec[:1000],set_vec[:1000])\n",
    "print(edm)\n",
    "matplotlib.pyplot.matshow(edm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比双十一数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"../data/double11_1020_1120.csv\")\n",
    "csv_data.fillna(0.0,inplace=True)\n",
    "csv_data *= 100.0\n",
    "csv_data_u = csv_data.round(5).drop_duplicates(subset=csv_data.columns[1:],keep='first')\n",
    "\n",
    "# csv_data_u = csv_data_u.sample(n=65536, frac=None, replace=False, weights=None, random_state=None, axis=0)\n",
    "csv_data_u_cut = csv_data_u.iloc[:,1:]\n",
    "csv_data_u_float = csv_data_u_cut.astype('float32')\n",
    "print(csv_data_u_float.shape)\n",
    "\n",
    "# for x in csv_data_u_float.duplicated():\n",
    "#     if (x is True):\n",
    "#         print(\"duplication exist\")\n",
    "#         break\n",
    "\n",
    "# 2进制数组\n",
    "with open(\"../data/eco_nodes\",'wb') as bin_output:\n",
    "    csv_data_u_float.values.tofile(bin_output)\n",
    "\n",
    "# with open(\"../Kluster/data/eco_nodes.csv\",'w') as csv_output:\n",
    "#     csv_data_u.to_csv(csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "node_num_c = csv_data_u_float.shape[0]\n",
    "node_dim_c = csv_data_u_float.shape[1]\n",
    "threshold_c = 0.1\n",
    "os.system(' '.join([\"cd ..; cd bin; ./linker ../data/eco_nodes ../data/eco_edges.csv\",str(node_num_c),str(node_dim_c),str(threshold_c)]))\n",
    "eco_edge = pd.read_csv(\"../Kluster/data/eco_edges.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_edge['distance'].hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "disjoint_set = read_set(\"../Kluster/data/set.txt\")\n",
    "biggest_set = 0\n",
    "bs_len = 1\n",
    "for set_id,node_list in disjoint_set.items():\n",
    "    if len(node_list)>bs_len:\n",
    "        biggest_set = set_id\n",
    "        bs_len = len(node_list)\n",
    "\n",
    "print(bs_len)\n",
    "print(disjoint_set[biggest_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_vec = [csv_data_u_float.iloc[vec_id] for vec_id in disjoint_set[biggest_set]]\n",
    "edm = ED(set_vec[:1000],set_vec[:1000])\n",
    "print(edm)\n",
    "matplotlib.pyplot.matshow(edm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
